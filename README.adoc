= Kafka Zero to Hero Workshop

This is a hands-on workshop that aims to give you basic practical experience with the main components of Kafka within the Kubernetes world using AMQ Streams, a Red Hat solution based on the Open Source https://strimz.io[Strimzi] project.

== Introduction

link:https://www.redhat.com/en/topics/integration/what-is-apache-kafka[Apache Kafka] has become the leading platform for building real-time data pipelines.  Today, Kafka is heavily used for developing event-driven applications,  where it lets services communicate with each other through events. Using Kubernetes for this type of workload requires adding specialized  components such as Kubernetes Operators and connectors to bridge the  rest of your systems and applications to the Kafka ecosystem.

To respond to business demands quickly and efficiently, you need a way  to integrate applications and data spread across your enterprise. link:https://www.redhat.com/en/technologies/jboss-middleware/amq[Red Hat AMQ] — based on open source communities like Apache ActiveMQ and Apache  Kafka — is a flexible messaging platform that delivers information  reliably, enabling real-time integration and connecting the Internet of  Things (IoT).

image::https://access.redhat.com/webassets/avalon/d/Red_Hat_AMQ-7.7-Evaluating_AMQ_Streams_on_OpenShift-en-US/images/320e68d6e4b4080e7469bea094ec8fbf/operators.png[Operators within the AMQ Streams architecture]

AMQ streams, a link:https://www.redhat.com/en/products/application-foundations[Red Hat Application Foundation] component, makes Apache Kafka “OpenShift native” through the use of  powerful operators that simplify the deployment, configuration,  management, and use of Apache Kafka on Red Hat OpenShift.

== Audience

- Developers
- Architects
- Data Integrators

=== Duration

This workshop introduces participants to AMQ through a presentation and  hands-on lab format. This workshop is intended to be completed in a  half-day (4hrs) session.

== Agenda

=== Concepts - Kafka 101

Duration: 90 minutes with demos

https://docs.google.com/presentation/d/1CI5eRojj2KkcG_KmZtsRr_N75Y8vFEM6g-EVrpqId1M[Presentation Deck] (Red Hat Internal Only)

. Event Driven Architecture
. Kafka
. Kafka Brokers
. Kafka High Availability
. Kafka Leaders / Replicas
. Kafka Mirror Maker
. Kafka Store
. Kafka Security
. Kafka Connect vs Camel
. Kafka on Kubernetes (Strimzi)

=== Labs

. link:./labs/0-to-60.adoc[AMQ Streams on OpenShift from 0 to 60]

. link:./labs/production-ready-topologies.adoc[Production-ready topologies: sizing and persistence]

. link:./labs/topic-management.adoc[Topic management]

. link:./labs/understanding-the-application-ecosystem.adoc[Understanding the Kafka application ecosystem]

. link:./labs/clients-within-outside-OCP.adoc[Clients: within OCP and outside OCP]

. link:./labs/security.adoc[Security]

. link:./labs/watching-multiple-namespaces-short-1.1.adoc[Watching multiple namespaces]

. link:./labs/mirror-maker.adoc[Replicating Data with MirrorMaker]

. link:./labs/kafka-connect.adoc[Running KafkaConnect applications]

. link:./labs/management-monitoring.adoc[Management and monitoring]

== Using the Ansible playbook

=== Parameters

[options="header"]
|=======================
| Parameter | Example Value                                      | Definition
| tkn | sha256~vFanQbthlPKfsaldJT3bdLXIyEkd7ypO_XPygY1DNtQ | Access token for a user with cluster-admin privileges.
| server    | https://api.cluster-xyz.xyz.sandbox9999.opentlc.com:6443                             | The cluster's OpenShift API URL
| num_users | 8                                                  | Number of users attending the workshop. Have in mind that the more users you have, more resources you are going to need from the cluster.
| delete_workshop | false | Only use this parameter if you intend to delete the installation and preserve the cluster.
|=======================

----
export tkn=sha256~UNFpoNDl1kPJ-nEhUzt-hljRpCBndpk66t1UycKj4Z4
export server=https://api.cluster-96npr.96npr.sandbox1420.opentlc.com:6443
export num_users=1
----

=== Running the playbook

- Replace the environment variable values:

----
export tkn=CHANGEME
export server=CHANGEME
export num_users=CHANGEME

cd ansible

ansible-playbook -e token=${tkn} -e server=${server} -e num_users=${num_users} playbook.yml
----

=== Getting the user registration page

The URL will be displayed by the `Installation Finished` task.

image::./images/user-distribution-console.PNG[User Distribution]

=== Contributing

We welcome all forms of contribution (content, issues/bugs, feedback).

=== Support and ownership

If you have any questions or are in need of support, reach out to link:https://github.com/hguerrero[Hugo Guerrero]

